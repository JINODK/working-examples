{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mortgage Qwiklab",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTry4ZMD2859",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to the What-If Tool Challenge Lab!\n",
        "\n",
        "In this notebook, you will use mortgage data from NY in 2017 to create two binary classifiers to determine if a mortgage applicant will be granted a loan.\n",
        "\n",
        "You will train a classifier on two datasets. One is trained on the complete dataset, while the other is trained on a subset of the dataset where 90% of the female applicants that were granted a loan were removed from the training data (so the training data has 90% less females that were granted loans).\n",
        "\n",
        "You will then compare the two models using the What-If Tool.\n",
        "\n",
        "\n",
        "Both models are trained on their respective datasets and then are compared using the What-If Tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU9bzX-VWQCb",
        "colab_type": "text"
      },
      "source": [
        "# Download and import the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq53nhEV-Mvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#You'll need to install XGBoost on the TF instance\n",
        "!pip install witwidget --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhmYvLmUxSqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "# from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "\n",
        "\n",
        "# Data from https://www.consumerfinance.gov/data-research/hmda/historic-data/?geo=ny&records=all-records&field_descriptions=labels\n",
        "!wget https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_ny_all-records_labels.zip\n",
        "\n",
        "!unzip hmda_2017_ny_all-records_labels.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFyKHeHZD1e6",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess the Data\n",
        "\n",
        "Here, we first import that dataset into a Pandas dataframe. Then we process the data to exlude incomplete information and make a simple binary classification of loan approvals. We then create two datasets, one complete and one where 90% of female applicants are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSsrdPdyCVYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set column dtypes for Pandas\n",
        "COLUMN_NAMES = collections.OrderedDict({\n",
        "  'as_of_year': np.int16,\n",
        "  'agency_abbr': 'category',\n",
        "  'loan_type': 'category',\n",
        "  'property_type': 'category',\n",
        "  'loan_purpose': 'category',\n",
        "  'owner_occupancy': np.int8,\n",
        "  'loan_amt_000s': np.float64,\n",
        "  'preapproval': 'category',\n",
        "  'county_code': np.float64,\n",
        "  'applicant_income_00s': np.float64,\n",
        "  'purchaser_type': 'category',\n",
        "  'hoepa_status': 'category',\n",
        "  'lien_status': 'category',\n",
        "  'population': np.float64,\n",
        "  'ffiec_median_fam_income': np.float64,\n",
        "  'tract_to_msamd_income': np.float64,\n",
        "  'num_of_owner_occupied_units': np.float64,\n",
        "  'number_of_1_to_4_family_units': np.float64,\n",
        "  'approved': np.int8, \n",
        "  'applicant_race_name_3': 'category',\n",
        "  'applicant_race_name_4': 'category',\n",
        "  'applicant_race_name_5': 'category',\n",
        "  'co_applicant_race_name_3': 'category',\n",
        "  'co_applicant_race_name_4': 'category',\n",
        "  'co_applicant_race_name_5': 'category'\n",
        "})\n",
        "\n",
        "# Import the CSV into a dataframe\n",
        "data = pd.read_csv('hmda_2017_ny_all-records_labels.csv', dtype=COLUMN_NAMES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWNJwq2-Htxz",
        "colab_type": "code",
        "outputId": "70e6b283-1e90-43ea-8ad2-f62c6aaa9d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Only use a subset of the columns for these models.\n",
        "text_columns_to_keep = [\n",
        "             'agency_name',\n",
        "             'loan_type_name',\n",
        "             'property_type_name',\n",
        "             'loan_purpose_name',\n",
        "             'owner_occupancy_name',\n",
        "             'applicant_ethnicity_name',\n",
        "             'applicant_race_name_1',\n",
        "             'applicant_sex_name',                      \n",
        "]\n",
        "numeric_columns_to_keep = [\n",
        "             'loan_amount_000s',\n",
        "             'applicant_income_000s',\n",
        "             'population',\n",
        "             'minority_population',\n",
        "             'hud_median_family_income' \n",
        "]\n",
        "columns_to_keep = text_columns_to_keep + numeric_columns_to_keep + ['action_taken_name']\n",
        "\n",
        "# Drop columns with incomplete information and drop columns that don't have loan orignated or denied, to make this a simple binary classification.\n",
        "df = data[columns_to_keep].dropna()\n",
        "binary_df = df[df.action_taken_name.isin(['Loan originated', 'Application denied by financial institution'])]\n",
        "binary_df['loan_granted']= np.where(binary_df['action_taken_name'] == 'Loan originated', 1, 0)\n",
        "\n",
        "\n",
        "binary_df = binary_df.drop(columns=['action_taken_name'])\n",
        "\n",
        "# Drop 90% of loaned female applicants for a \"bad training data\" version.\n",
        "loaned_females = (binary_df['applicant_sex_name'] == 'Female') & (binary_df['loan_granted'] == 1)\n",
        "bad_binary_df = binary_df.drop(binary_df[loaned_females].sample(frac=.9).index)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic6mWTvENrLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label preprocessing\n",
        "labels = binary_df['loan_granted'].values\n",
        "\n",
        "# See the distribution of approved / denied classes (0: denied, 1: approved)\n",
        "print(binary_df['loan_granted'].value_counts())\n",
        "\n",
        "# Check out the head \n",
        "print(\"binary_df head: \",binary_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3kQmIqMLYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn categorical string features into simple 0/1 features (like turning \"sex\" into \"sex_male\" and \"sex_female\")\n",
        "dummies_df = pd.get_dummies(binary_df, columns=text_columns_to_keep)\n",
        "dummies_df = dummies_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "bad_dummies_df = pd.get_dummies(bad_binary_df, columns=text_columns_to_keep)\n",
        "bad_dummies_df = bad_dummies_df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfdY4PzWOoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the numeric columns so that the all have the same scale to simplify modeling/training.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def normalize():\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  # wouldn't these values be gone tho\n",
        "  column_names_to_normalize = ['loan_amount_000s', 'applicant_income_000s', 'minority_population', 'hud_median_family_income', 'population']\n",
        "  x = dummies_df[column_names_to_normalize].values\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = dummies_df.index)\n",
        "  dummies_df[column_names_to_normalize] = df_temp\n",
        "\n",
        "  x = bad_dummies_df[column_names_to_normalize].values\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  bad_df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = bad_dummies_df.index)\n",
        "  bad_dummies_df[column_names_to_normalize] = bad_df_temp\n",
        "\n",
        "normalize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np8JM4KINnKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Get training data & labels\n",
        "train_data = dummies_df\n",
        "train_labels = train_data['loan_granted']\n",
        "train_data = train_data.drop(columns=['loan_granted'])\n",
        "\n",
        "# Get bad training data and labels\n",
        "bad_train_data = bad_dummies_df\n",
        "bad_train_labels = bad_train_data['loan_granted']\n",
        "bad_train_data = bad_dummies_df.drop(columns=['loan_granted'])\n",
        "\n",
        "# Split the data into train / test sets\n",
        "x,y = train_data,train_labels\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
        "\n",
        "# Split the bad data into train / test sets\n",
        "bad_x,bad_y=bad_train_data,bad_train_labels\n",
        "bad_x_train,bad_x_test,bad_y_train,bad_y_test = train_test_split(bad_x,bad_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxItgKCJ-EEV",
        "colab_type": "code",
        "outputId": "357e960f-8cbe-4b0b-a2e2-445b5e11ec8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train the model, this will take a few minutes to run\n",
        "model_1 = xgb.XGBClassifier(max_depth=3, learning_rate=0.05, objective='reg:logistic')\n",
        "\n",
        "model_2 = xgb.XGBClassifier(objective='reg:logistic',max_depth=3)\n",
        "\n",
        "model_1.fit(x_train, y_train)\n",
        "\n",
        "model_2.fit(bad_x_train, bad_y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='reg:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0Az4X5fA6BS",
        "colab_type": "code",
        "outputId": "bf725a61-a644-49bc-e82d-f6d9a8ab8502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Get predictions on the test set and print the accuracy score (Model 1)\n",
        "y_pred = model_1.predict(x_test)\n",
        "acc = accuracy_score(y_test, y_pred.round())\n",
        "print(\"accuracy of model_1 \", acc, '\\n')\n",
        "\n",
        "# Print a confusion matrix for Model 1\n",
        "print('Confusion matrix for Model 1:')\n",
        "cm = confusion_matrix(y_test, y_pred.round())\n",
        "cm = cm / cm.astype(np.float).sum(axis=1)\n",
        "print(cm, '\\n')\n",
        "\n",
        "# Get predictions on the test set and print the accuracy score (Model 2)\n",
        "bad_y_pred = model_2.predict(bad_x_test)\n",
        "acc = accuracy_score(bad_y_test, bad_y_pred.round())\n",
        "print(\"accuracy of model_2 \", acc, '\\n')\n",
        "\n",
        "# Print a confusion matrix for Model 2\n",
        "print('Confusion matrix for Model 2:')\n",
        "cm = confusion_matrix(bad_y_test, bad_y_pred.round())\n",
        "cm = cm / cm.astype(np.float).sum(axis=1)\n",
        "print(cm)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy of model_1  0.7974464038485742 \n",
            "\n",
            "Confusion matrix for Model 1:\n",
            "[[0.15173337 0.23848428]\n",
            " [0.07475147 0.97898414]] \n",
            "\n",
            "accuracy of model_2  0.7977705861201007 \n",
            "\n",
            "Confusion matrix for Model 2:\n",
            "[[0.43128655 0.22430305]\n",
            " [0.1462624  0.94231348]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj0FGG1Y-f3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model so we can deploy it\n",
        "model_1.save_model('good-model.bst')\n",
        "\n",
        "model_2.save_model('bad-model.bst')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfp8H0esC6k_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GCP_PROJECT = ''\n",
        "MODEL_BUCKET = ''\n",
        "MODEL_NAME = 'good-model.bst' # You'll create this model below\n",
        "BAD_MODEL_NAME = 'bad-model.bst'\n",
        "VERSION_NAME = 'v1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJOTCAsLDjcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copy your model file to Cloud Storage\n",
        "!gsutil cp ./good-model.bst $MODEL_BUCKET\n",
        "\n",
        "!gsutil cp ./bad-model.bst $MODEL_BUCKET"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbGP-3qIDoza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure gcloud to use your project\n",
        "!gcloud config set project $GCP_PROJECT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSfwEaE8DpOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a model\n",
        "!gcloud ai-platform models create $MODEL_NAME"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKJR-7i5DpY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a version, this will take ~2 minutes to deploy\n",
        "!gcloud ai-platform versions create $VERSION_NAME \\\n",
        "--model=$MODEL_NAME \\\n",
        "--framework='XGBOOST' \\\n",
        "--runtime-version=1.14 \\\n",
        "--origin=$MODEL_BUCKET \\\n",
        "--staging-bucket=$MODEL_BUCKET \\\n",
        "--python-version=3.5 \\\n",
        "--project=$GCP_PROJECT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIEzd20lrC25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a version, this will take ~2 minutes to deploy\n",
        "!gcloud ai-platform versions create $VERSION_NAME \\\n",
        "--model=$BAD_MODEL_NAME \\\n",
        "--framework='XGBOOST' \\\n",
        "--runtime-version=1.14 \\\n",
        "--origin=$MODEL_BUCKET \\\n",
        "--staging-bucket=$MODEL_BUCKET \\\n",
        "--python-version=3.5 \\\n",
        "--project=$GCP_PROJECT"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IZAJ1LrqUha",
        "colab_type": "text"
      },
      "source": [
        "# Using the What-if Tool to interpret your model\n",
        "Once your model has deployed, you're ready to connect it to the What-if Tool using the WitWidget."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y0tKiINqWnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Format a subset of the test data to send to the What-if Tool for visualization\n",
        "# Append ground truth label value to training data\n",
        "\n",
        "# This is the number of examples you want to display in the What-if Tool\n",
        "num_wit_examples = 500\n",
        "test_examples = np.hstack((x_test[:num_wit_examples].values,y_test[:num_wit_examples].reshape(-1,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4PPaGsfqacx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a What-if Tool visualization, it may take a minute to load\n",
        "# See the cell below this for exploration ideas\n",
        "\n",
        "# This prediction adjustment function is needed as this xgboost model's\n",
        "# prediction returns just a score for the positive class of the binary\n",
        "# classification, whereas the What-If Tool expects a list of scores for each\n",
        "# class (in this case, both the negative class and the positive class).\n",
        "def adjust_prediction(pred):\n",
        "  return [1 - pred, pred]\n",
        "\n",
        "config_builder = (WitConfigBuilder(test_examples.tolist(), data.columns.tolist() + ['mortgage_status'])\n",
        "  .set_ai_platform_model(GCP_PROJECT, MODEL_NAME, VERSION_NAME, adjust_prediction=adjust_prediction)\n",
        "  .set_target_feature('mortgage_status')\n",
        "  .set_label_vocab(['denied', 'approved']))\n",
        "WitWidget(config_builder, height=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQrAb7lbOhvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install witwidget --quiet\n",
        "#!pip3 install witwidget\n",
        "\n",
        "#@title Show model results in WIT\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "num_datapoints = 1000  #@param {type: \"number\"}\n",
        "\n",
        "# Column indices to strip out from data from WIT before passing it to the model.\n",
        "columns_not_for_model_input = [\n",
        "    test_data_with_labels.columns.get_loc('loan_granted'),\n",
        "]\n",
        "\n",
        "# Return model predictions.\n",
        "def custom_predict(examples_to_infer):\n",
        "  # Delete columns not used by model\n",
        "  model_inputs = np.delete(\n",
        "      np.array(examples_to_infer), columns_not_for_model_input, axis=1).tolist()\n",
        "  # Get the class predictions from the model.\n",
        "  preds = model.predict(model_inputs)\n",
        "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
        "  return preds\n",
        "  \n",
        "def bad_custom_predict(examples_to_infer):\n",
        "  # Delete columns not used by model\n",
        "  model_inputs = np.delete(\n",
        "      np.array(examples_to_infer), columns_not_for_model_input, axis=1).tolist()\n",
        "\n",
        "  # Get the class predictions from the model.\n",
        "  preds = bad_model.predict(model_inputs)\n",
        "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
        "\n",
        "  return preds\n",
        "\n",
        "examples_for_wit = test_data_with_labels.values.tolist()\n",
        "column_names = test_data_with_labels.columns.tolist()\n",
        "\n",
        "config_builder = WitConfigBuilder(\n",
        "    examples_for_wit[:num_datapoints],\n",
        "    feature_names=column_names).set_custom_predict_fn(\n",
        "  bad_custom_predict).set_target_feature('loan_granted').set_label_vocab(\n",
        "      ['denied', 'accepted']).set_compare_custom_predict_fn(custom_predict).set_model_name('limited').set_compare_model_name('complete')\n",
        "\n",
        "ww = WitWidget(config_builder, height=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT3Mbq3G3f8l",
        "colab_type": "text"
      },
      "source": [
        "Things to notice:\n",
        "- In the datapoint visualization, the top arc of points (not on the diagonal) are the females in the test data, where the limited dataset model under-scores female applicants compared to the complete dataset model. You can see this clearly by binning or coloring the visualization by sex.\n",
        "- In the performance & fairness tab, the complete model has much higher accuracy and f1 score.\n",
        "- If you slice by sex, the complete model has equal performance across sexes, whereas the limited model is much, much worse on females.\n",
        "  - If you use the fairness buttons to see the thresholds for the sexes for demographic parity between male and female, you see that the thresholds have to be wildly different for the limited model."
      ]
    }
  ]
}