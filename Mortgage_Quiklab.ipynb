{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mortgage Quiklab",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rocpoc/demo-repo/blob/master/Mortgage_Quiklab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTry4ZMD2859",
        "colab_type": "text"
      },
      "source": [
        "# Welcome to the What-If Tool Challenge Lab!\n",
        "\n",
        "In this notebook, you will use mortgage data from NY in 2017 to create two binary classifiers to determine if a mortgage applicant will be granted a loan.\n",
        "\n",
        "You will train a classifier on two datasets. One is trained on the complete dataset, while the other is trained on a subset of the dataset where 90% of the female applicants that were granted a loan were removed from the training data (so the training data has 90% less females that were granted loans).\n",
        "\n",
        "You will then compare the two models using the What-If Tool.\n",
        "\n",
        "\n",
        "Both models are trained on their respective datasets and then are compared using the What-If Tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU9bzX-VWQCb",
        "colab_type": "text"
      },
      "source": [
        "# First, download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhmYvLmUxSqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data from https://www.consumerfinance.gov/data-research/hmda/historic-data/?geo=ny&records=all-records&field_descriptions=labels\n",
        "!wget https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_ny_all-records_labels.zip\n",
        "!unzip hmda_2017_ny_all-records_labels.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSsrdPdyCVYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = pd.read_csv('hmda_2017_ny_all-records_labels.csv',)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pvv1mr_9DfMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWNJwq2-Htxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Only use a subset of the columns for these models.\n",
        "text_columns_to_keep = [\n",
        "             'agency_name',\n",
        "             'loan_type_name',\n",
        "             'property_type_name',\n",
        "             'loan_purpose_name',\n",
        "             'owner_occupancy_name',\n",
        "             'applicant_ethnicity_name',\n",
        "             'applicant_race_name_1',\n",
        "             'applicant_sex_name',                      \n",
        "]\n",
        "numeric_columns_to_keep = [\n",
        "             'loan_amount_000s',\n",
        "             'applicant_income_000s',\n",
        "             'population',\n",
        "             'minority_population',\n",
        "             'hud_median_family_income' \n",
        "]\n",
        "columns_to_keep = text_columns_to_keep + numeric_columns_to_keep + ['action_taken_name']\n",
        "\n",
        "# Drop columns with incomplete information and drop columns that don't have loan orignated or denied, to make this a simple binary classification.\n",
        "df = data[columns_to_keep].dropna()\n",
        "binary_df = df[df.action_taken_name.isin(['Loan originated', 'Application denied by financial institution'])]\n",
        "binary_df['loan_granted']= np.where(binary_df['action_taken_name'] == 'Loan originated', 1, 0)\n",
        "\n",
        "\n",
        "binary_df = binary_df.drop(columns=['action_taken_name'])\n",
        "\n",
        "# Drop 90% of loaned female applicants for a \"bad training data\" version.\n",
        "loaned_females = (binary_df['applicant_sex_name'] == 'Female') & (binary_df['loan_granted'] == 1)\n",
        "bad_binary_df = binary_df.drop(binary_df[loaned_females].sample(frac=.9).index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3kQmIqMLYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn categorical string features into simple 0/1 features (like turning \"sex\" into \"sex_male\" and \"sex_female\")\n",
        "dummies_df = pd.get_dummies(binary_df, columns=text_columns_to_keep)\n",
        "dummies_df = dummies_df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "bad_dummies_df = pd.get_dummies(bad_binary_df, columns=text_columns_to_keep)\n",
        "bad_dummies_df = bad_dummies_df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VfdY4PzWOoI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize the numeric columns so that the all have the same scale to simplify modeling/training.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def normalize():\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  column_names_to_normalize = ['loan_amount_000s', 'applicant_income_000s', 'minority_population', 'hud_median_family_income', 'population']\n",
        "  x = dummies_df[column_names_to_normalize].values\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = dummies_df.index)\n",
        "  dummies_df[column_names_to_normalize] = df_temp\n",
        "\n",
        "  x = bad_dummies_df[column_names_to_normalize].values\n",
        "  x_scaled = min_max_scaler.fit_transform(x)\n",
        "  bad_df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = bad_dummies_df.index)\n",
        "  bad_dummies_df[column_names_to_normalize] = bad_df_temp\n",
        "\n",
        "normalize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np8JM4KINnKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the data into test and train datasets.\n",
        "train_size = int(len(dummies_df) * 0.8)\n",
        "\n",
        "# train_data = dummies_df[:train_size]\n",
        "train_data = dummies_df\n",
        "train_labels = train_data['loan_granted']\n",
        "train_data = train_data.drop(columns=['loan_granted'])\n",
        "\n",
        "bad_train_size = int(len(bad_dummies_df) * 0.8)\n",
        "# bad_train_data = bad_dummies_df[:bad_train_size]\n",
        "bad_train_data = bad_dummies_df\n",
        "bad_train_labels = bad_train_data['loan_granted']\n",
        "bad_train_data = bad_train_data.drop(columns=['loan_granted'])\n",
        "\n",
        "test_data_with_labels = dummies_df[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX0S3p3IMsoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "import collections\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Split the data into train / test sets\n",
        "x,y = train_data,train_labels\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y)\n",
        "\n",
        "# Train the model, this will take a few minutes to run\n",
        "bst = xgb.XGBClassifier(\n",
        "    objective='reg:logistic'\n",
        ")\n",
        "\n",
        "bst.fit(x_train, y_train)\n",
        "# Get predictions on the test set and print the accuracy score\n",
        "y_pred = bst.predict(x_test)\n",
        "acc = accuracy_score(y_test, y_pred.round())\n",
        "print(\"complete model accuracy:\", acc, '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rxHnqADJqWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the bad data into train / test sets\n",
        "bad_x,bad_y = bad_train_data,bad_train_labels\n",
        "bad_x_train,bad_x_test,bad_y_train,bad_y_test = train_test_split(bad_x,bad_y)\n",
        "\n",
        "# Train the model, this will take a few minutes to run\n",
        "bst = xgb.XGBClassifier(\n",
        "    objective='reg:logistic'\n",
        ")\n",
        "\n",
        "bst.fit(bad_x_train, bad_y_train)\n",
        "# Get predictions on the test set and print the accuracy score\n",
        "y_pred = bst.predict(bad_x_test)\n",
        "acc = accuracy_score(bad_y_test, y_pred.round())\n",
        "print(\"bad model accuracy:\", acc, '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCxdKRrJOSd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.constraints import maxnorm\n",
        "\n",
        "# This is the size of the array we'll be feeding into our model for each example\n",
        "input_size = len(train_data.iloc[0])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(64, input_shape=(input_size,), activation='relu', kernel_constraint=maxnorm(3)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# model.add(layers.Dense(100, input_dim=input_size, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "# # hidden layer\n",
        "# model.add(layers.Dense(60, activation='relu', kernel_constraint=maxnorm(3)))\n",
        "# model.add(layers.Dropout(0.2))\n",
        "# # output layer\n",
        "# model.add(layers.Dense(1, activation='softmax'))\n",
        "# # Compile model\n",
        "# model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(train_data.values, train_labels.values, epochs=10, batch_size=2048, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha_UlPuwzPNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85cXfpR7cgzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model for the bad training data\n",
        "bad_model = Sequential()\n",
        "bad_model.add(layers.Dense(200, input_shape=(input_size,), activation='relu'))\n",
        "bad_model.add(layers.Dense(50, activation='relu'))\n",
        "bad_model.add(layers.Dense(20, activation='relu'))\n",
        "bad_model.add(layers.Dense(1, activation='sigmoid'))\n",
        "bad_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "bad_model.fit(bad_train_data.values, bad_train_labels.values, epochs=10, batch_size=2048, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB3a47ssO2Ob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install witwidget --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQrAb7lbOhvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Show model results in WIT\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "num_datapoints = 1000  #@param {type: \"number\"}\n",
        "\n",
        "# Column indices to strip out from data from WIT before passing it to the model.\n",
        "columns_not_for_model_input = [\n",
        "    test_data_with_labels.columns.get_loc('loan_granted'),\n",
        "]\n",
        "\n",
        "# Return model predictions.\n",
        "def custom_predict(examples_to_infer):\n",
        "  # Delete columns not used by model\n",
        "  model_inputs = np.delete(\n",
        "      np.array(examples_to_infer), columns_not_for_model_input, axis=1).tolist()\n",
        "  # Get the class predictions from the model.\n",
        "  preds = model.predict(model_inputs)\n",
        "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
        "  return preds\n",
        "  \n",
        "def bad_custom_predict(examples_to_infer):\n",
        "  # Delete columns not used by model\n",
        "  model_inputs = np.delete(\n",
        "      np.array(examples_to_infer), columns_not_for_model_input, axis=1).tolist()\n",
        "\n",
        "  # Get the class predictions from the model.\n",
        "  preds = bad_model.predict(model_inputs)\n",
        "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
        "\n",
        "  return preds\n",
        "\n",
        "examples_for_wit = test_data_with_labels.values.tolist()\n",
        "column_names = test_data_with_labels.columns.tolist()\n",
        "\n",
        "config_builder = WitConfigBuilder(\n",
        "    examples_for_wit[:num_datapoints],\n",
        "    feature_names=column_names).set_custom_predict_fn(\n",
        "  bad_custom_predict).set_target_feature('loan_granted').set_label_vocab(\n",
        "      ['denied', 'accepted']).set_compare_custom_predict_fn(custom_predict).set_model_name('limited').set_compare_model_name('complete')\n",
        "\n",
        "ww = WitWidget(config_builder, height=800)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT3Mbq3G3f8l",
        "colab_type": "text"
      },
      "source": [
        "Things to notice:\n",
        "- In the datapoint visualization, the top arc of points (not on the diagonal) are the females in the test data, where the limited dataset model under-scores female applicants compared to the complete dataset model. You can see this clearly by binning or coloring the visualization by sex.\n",
        "- In the performance & fairness tab, the complete model has much higher accuracy and f1 score.\n",
        "- If you slice by sex, the complete model has equal performance across sexes, whereas the limited model is much, much worse on females.\n",
        "  - If you use the fairness buttons to see the thresholds for the sexes for demographic parity between male and female, you see that the thresholds have to be wildly different for the limited model."
      ]
    }
  ]
}